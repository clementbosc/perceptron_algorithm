#!/usr/bin/env python
# -*- coding: utf-8 -*-
# prerequis : librairie pylab installée (regroupe scypy, matplotlib, numpy et ipython)
# codage de la méthode du perceptron pour la classification binaire linéairement séparable 

from pylab import *
import random
import pdb

fig = figure()


def attente_touche():
    raw_input()


def affichage_perceptron(data1, data2, w):
    assert type(data1) is list
    assert type(data2) is list
    assert type(w) is list
    fig.clear()
    forme1 = 'b+'
    forme2 = 'r*'
    hold(True)
    for point in data1:
        plot(point[0], point[1], forme1)
    # break
    for point in data2:
        plot(point[0], point[1], forme2)
    vectAxes = axis()
    # dans le cas ou l'on ajoute le terme d'erreur en fin de liste : point = point + [1.0]
    plot([vectAxes[0],vectAxes[1]],[-(vectAxes[0]*w[0]+w[2])/w[1],-(vectAxes[1]*w[0]+w[2])/w[1]],'m:')

    # dans le cas ou l'on ajoute le terme d'erreur en début de liste : point = [1.0] + point
    # plot([vectAxes[0], vectAxes[1]], [-(vectAxes[0] * w[1] + w[0]) / w[2], -(vectAxes[1] * w[1] + w[0]) / w[2]], 'm:')

    # axis(vectAxes)
    fig.show()
    hold(False)


def perceptron(dataGroupe1, dataGroupe2):
    assert type(dataGroupe1) is list
    assert type(dataGroupe2) is list
    #ion()
    # perceptron recherche l'hyperplan séparateur entre les deux classes de points
    # avec mu qui represente la vitesse de descente. Il renvoi la liste w (w0, w1, w2)
    # des coefficients de l'équation de l'hyperplan, tel que w.x\=0
    #
    # Entrée :
    # - dataGroupe1 : liste de point (1 point = 1 vecteur de dimension 2) de la classe 1
    # - dataGroupe2 : liste de point (1 point = 1 vecteur de dimension 2) de la classe 2
    # - mu : coefficient de mise a jour, w(n) = w(n-1)+mu*sigma(delta(y)*y\)
    # Sortie :
    # - w : coefficients de l'hyperplan séparateur

    # initialisation quelconque de w

    nDim = len(dataGroupe1[0])
    w = [1.] * (nDim + 1)

    # descente du gradient
    non_stabilise = True
    iteration = 1
    while non_stabilise:
        acc = [0.] * (nDim + 1)
        # classe 1 : classe positive (point.w > 0)
        delta = 1
        for point in dataGroupe1:
            # ajout terme d'erreur : point = [1., x, y]
            point = point + [1.0]

            # calcul du produit scalaire entre le point et w
            prod = np.vdot(point, w)

            # accumulation de delta * point pour les points mal classés
            if prod <= 0:
                acc = calc_acc(acc, delta, point)

            del point[nDim]  # pour ne pas ajouter 1.0 au point à chaque boucle

        # classe 2 : classe négative (point.w < 0) donc on prend delta=-1.
        delta = -1  # etiquette
        for point in dataGroupe2:
            # ajout terme d'erreur au point : point = [x, y, 1.]
            point = point + [1.0]

            # calcul du produit scalaire entre le point et w
            prod = np.vdot(point, w)

            # accumulation de delta * point pour les points mal classés
            if prod > 0:
                acc = calc_acc(acc, delta, point)

            del point[nDim]  # pour ne pas ajouter 1.0 au point à chaque boucle

        # mise a jour de w. indication : prendre mu=1./iteration
        mu = 1. / iteration

        # calcul du nouveau W
        for i in range(len(w)):
            w[i] += (mu * acc[i])

        # mise a jour du compteur d'iteration
        iteration += 1

        sommAcc = 0
        for i in range(len(acc)):
            sommAcc += acc[i]

        # mise a jour de la condition de sortie
        if sommAcc == 0 or iteration > 100:
            non_stabilise = False

    return [w, iteration]


def calc_acc(acc, delta, point):
    for i in range(len(acc)):
        acc[i] += delta * point[i]
    return acc


def compute(c1, c2):
    w_res, iter = perceptron(c1, c2)
    affichage_perceptron(c1, c2, w_res)

    print("%d itérations" %(iter))
    print(w_res)

classe1 = [[3.6484028273127254, 0.073429844289042881], [2.330634259457685, 0.21748520551489497],
           [2.644948143301538, 0.20411304520644272], [2.7753121336336251, 0.18681685779827709],
           [2.1983916903402347, 0.19693911318338075], [2.836900928011457, 0.16884540288894506],
           [2.6235769902396133, 0.14047193438597244], [2.7157154911382348, 0.18092519035140286],
           [2.6662657025141492, 0.17384029751490698], [2.7900235283901487, 0.17124467162767718],
           [2.3025641883523864, 0.19080233853944761], [2.50402381354598, 0.26502653237173324],
           [2.4698500051963022, 0.23443828139261397], [2.5051902610502612, 0.23067614484914289],
           [2.4684409160318621, 0.21863502590393655], [3.0295456522668971, 0.086190046340667389],
           [2.4470385350120138, 0.21074952985853601], [2.5841314160351514, 0.1399509161051439],
           [2.9340821512886901, 0.13392607949486274], [2.4656386533778831, 0.18075367321631461],
           [3.0586627888074345, 0.10824029230772414], [2.9672072605149942, 0.090654073097631682],
           [2.775570432066623, 0.10086794535765123], [3.2191814506505918, 0.091192361470483291],
           [3.0953915703017412, 0.083350309705367873], [2.7482148964142703, 0.16942149706399512],
           [3.2027270910895678, 0.087918202824296091], [2.4360700766475203, 0.18235455569032583],
           [3.0896110542649828, 0.070454584566214534], [3.1576771353124555, 0.02505081330045875],
           [3.2048019309354552, 0.099632924683255852], [3.2886739761802324, 0.11512859820039387],
           [3.3358952062041407, 0.066145702274442011], [2.7828193132133867, 0.10865301821665886],
           [3.2166810087946258, 0.06158458393059911], [3.4826860465180727, 0.054030442500088642],
           [2.6630896942975384, 0.072405436921074262], [3.2584377174878356, 0.0027596439083962801],
           [3.4835061798032245, 0.020834890108359642], [2.7437104039988292, 0.1596260983402053],
           [3.0420286983507383, 0.18487250264557131], [3.3679605095723288, 0.034126200184808955],
           [2.9496376092411056, 0.081494106624779794], [3.276460095937991, 0.098407358447723581],
           [3.4357109495848381, 0.0014493704542924747], [3.1736704044362374, 0.077759877465631635],
           [3.1434499936505738, 0.078734145799675637], [3.1428899696402128, 0.060845911720905574],
           [2.9379529036560368, 0.045547161283897034], [3.2089924934429574, 0.12283155110475004],
           [3.4542746667763367, 0.10785672077705655], [2.7965926154547343, 0.14642144294521434],
           [2.6649571429239129, 0.14933978450365989], [3.1919733970311075, 0.044802044590003448],
           [3.0813841971195908, 0.12421763215801214], [2.911782496786496, 0.11498266641726596],
           [3.1180106369112326, 0.13487735338007661], [2.9664191163511973, 0.14218734197333277],
           [3.19492557458006, 0.13622467213640552], [3.0311917767085572, 0.075204544086666603],
           [2.7545181362987505, 0.094162414567434835], [3.0356037948169061, 0.078296363162563734],
           [3.4479320223337884, 0.10244429339964044], [3.0357314745763695, 0.10721907017378812],
           [3.0345136320712607, 0.11424037457447507], [3.328999068492819, 0.029439223998972253],
           [2.5251530550192007, 0.099339586664086432], [3.4293009182112995, 0.091517332201219478],
           [3.2733924344784033, 0.068591172300513883], [3.0418312421871145, 0.14696283959922296],
           [3.1923599512528593, 0.1368594147939993], [3.7540448089417908, 0.0067096669694060081],
           [3.0836975550338934, 0.022960858046414481], [2.9758452235543573, 0.049060241098049823],
           [3.0306633687975308, 0.06850058672449881], [3.1703163460931409, 0.02622419557702417],
           [3.4624362222397793, 0.0013444878444853437], [2.8070519242256196, 0.043663018758445214],
           [3.1829288775437412, 0.066819848757048522], [3.4765665819593332, 0.097892770377636898],
           [3.0299364237682695, 0.069933489299203899], [3.317015178997988, 0.075534389783419101],
           [3.5005416645241167, 0.067763180666095921], [3.334179670043246, 0.090593469435488072],
           [2.9989767495329582, 0.11522548011247739], [3.3041557733745104, 0.053077232367796018],
           [2.7488052999496806, 0.14243961994376458], [3.4021643755794604, 0.062377026400570801],
           [3.5804031905008973, 0.080527230529664123], [2.5189452448579193, 0.14358842634613089],
           [3.1110632589288345, 0.13841104119853526], [3.382074896103834, 0.073852642356949938],
           [2.6078574423275191, 0.11264956698738329], [3.1859043918233758, 0.077241364391544512],
           [3.5047266746022538, 0.051199692315054952], [2.7874992286931035, 0.065705677381846156],
           [3.1412766943991577, 0.097455162475272833], [3.3273474374097347, 0.069212618313417754],
           [2.8821948558189998, 0.11055604699659106], [3.2169846180125758, 0.087140859591910008]]
classe2 = [[4.0958394694071085, 0.62090992691837554], [3.0515154047252695, 0.53512517722257091],
           [2.9491682603712408, 0.44028070952077253], [3.2829314223632156, 0.49129132399409814],
           [3.2997815280896532, 0.46202148420611983], [3.1994813507553297, 0.51730303394481048],
           [3.2739397765597413, 0.42456810719843918], [3.0094501931685627, 0.53307647477381548],
           [3.1121716887339406, 0.61264659221630813], [3.1567904507066951, 0.56034384641028545],
           [3.1666032722891706, 0.47572772737933283], [2.8978523007369641, 0.39472962675440465],
           [3.3719622811831127, 0.57256856612568585], [3.0025688112273086, 0.5581421242147987],
           [3.2243758188595679, 0.52064997498366095], [3.0110164875805565, 0.43111214123321362],
           [3.1485374067025722, 0.64330361652352819], [3.2302342511064359, 0.62838600861174609],
           [3.4725953755453878, 0.43200027260585433], [2.9714578972244743, 0.49869525827038585],
           [3.0911780469616459, 0.47262698060691322], [3.155431881696126, 0.58993887274267809],
           [3.0911785034790888, 0.48033791404377901], [3.294738852752884, 0.51416760228750735],
           [3.3532141357362542, 0.45551977982766856], [3.2055425094131733, 0.61076929173493655],
           [3.3561268899013381, 0.63904024192786979], [3.3027731782805279, 0.64401934183893794],
           [3.1684115830518, 0.50501912843936969], [3.482795130008816, 0.44040775725676201],
           [3.4080394302246613, 0.59565015920126507], [3.3895175732513585, 0.62145588763670512],
           [3.3369620491874583, 0.52638138748683161], [3.4365355959010655, 0.6287038124891563],
           [3.3580330759384229, 0.6244041060508132], [3.1829051937520547, 0.5685539468883708],
           [3.1971524118218104, 0.55328618523679163], [3.4318204645781205, 0.55025759284741271],
           [3.6261241592809204, 0.66982895020421873], [3.1248434427285066, 0.41087316716386951],
           [3.1583346875293969, 0.51770061554561642], [3.2071760952911039, 0.53136633951442436],
           [3.2623090537774635, 0.62609494577809721], [3.2461069764375359, 0.4642607326124103],
           [3.6879541577452875, 0.67707604637489682], [2.9977577385806122, 0.51757708680594383],
           [3.2684073147873525, 0.60513914948077208], [3.4701942597777382, 0.53056200829048417],
           [2.9591033633151858, 0.54350052993397713], [3.5656999827530349, 0.63044859102226325],
           [3.5965844587183518, 0.70638010744535396], [2.8736996243349404, 0.53946300127783131],
           [3.1556753403513587, 0.55615037361248165], [3.3129356374634491, 0.40844338584239992],
           [3.0732145708438212, 0.42626539200915231], [3.0345323817255343, 0.58139114386948132],
           [3.641953319221237, 0.58127118212253592], [3.2444619718903889, 0.571922168067167],
           [3.2822079445535195, 0.54728720875641912], [3.4141293003055031, 0.49514939631763749],
           [3.3280758673304232, 0.57736588007856182], [3.1742814818180869, 0.46443142335339538],
           [3.5348709621792955, 0.53139872171902003], [3.1908138474629517, 0.524435727481256],
           [3.4204928212779642, 0.55173818161339327], [3.6069393045623945, 0.58406346529627262],
           [3.1329599642758157, 0.66311256634454041], [3.2138078595370949, 0.67774494060951307],
           [3.5058600563949014, 0.64435807499551567], [3.5571075545964543, 0.67432399542265486],
           [3.3618324732742311, 0.46768753975786831], [3.5441993140972401, 0.6833229002492982],
           [3.2065990933586881, 0.39448137107966563], [3.3377370924970067, 0.49488043581445379],
           [3.3044478563735491, 0.55980398969054534], [3.3622932142887474, 0.63557338077559145],
           [3.6111974778917362, 0.45460478550977523], [3.3254362957160493, 0.61949503425908881],
           [3.4736812869272899, 0.59195336605167026], [3.3785354822897311, 0.61025115322403811],
           [3.3156657804150163, 0.67327144428452557], [3.5055202161083217, 0.65354479095763118],
           [3.6429534159916557, 0.6607066701999651], [3.0455256998614249, 0.61520518049456729],
           [3.319340784478884, 0.57771313014626269], [3.3613988365032306, 0.64804314186418355],
           [2.8689300507035944, 0.55970939975770284], [3.3401170164821536, 0.64935018556761459],
           [3.6614694993021746, 0.68532080485354652], [3.2065584428399818, 0.53760740573542409],
           [3.2891812822397717, 0.62563653700083799], [3.6798831614656944, 0.48137747046240176],
           [3.5595797516076821, 0.6218276060451019], [3.463841554916224, 0.66777574493147185],
           [3.8713806722576476, 0.68360413755682881], [3.1683159980453834, 0.5935805050643953],
           [3.4205441462453812, 0.4619086843212572], [3.7932660962185869, 0.66074722512640405],
           [3.2777804381821816, 0.58117619990930403], [3.4072203829365901, 0.54103284678991159]]
classe3 = [[3.836285556162859, 0.17488885815676444], [3.9046036496933407, 0.21995742377870348],
           [4.1545108753116367, 0.19645400534459045], [4.1987833316122352, 0.19703304602057539],
           [3.8715397281540196, 0.18336748445715684], [4.1631952751744903, 0.21583751780035143],
           [3.8067755719682346, 0.13029984504649231], [4.2331995779590086, 0.16583095278355137],
           [4.0664834258300173, 0.26180733777608278], [3.8539747179857335, 0.17215241559346361],
           [4.1093448646782491, 0.17765106796797839], [3.7176133236929481, 0.24048737653006386],
           [4.3310642505061496, 0.18847876748651521], [3.5832847883882, 0.22498735527596694],
           [3.999611132714854, 0.12367136651967732], [4.2664222137560586, 0.28957677201563969],
           [3.9681326235446828, 0.21086457727627866], [3.7197425382728775, 0.24458119801735781],
           [3.6350175478315383, 0.23427148092916744], [4.2094065783948853, 0.25841779979898372],
           [4.3815528175248355, 0.24543142687009481], [4.0572113256735518, 0.19196612796564255],
           [4.2207258198937829, 0.22207928757355119], [4.2013810415529758, 0.21985051181366944],
           [4.3788901983090529, 0.19439991875317839], [4.2345038863172295, 0.26591382174257949],
           [4.5561005383155964, 0.33223590242048656], [4.576926544830024, 0.2914982314320923],
           [4.5762902369030956, 0.34286332664636077], [4.4853140979806607, 0.28691236263903774],
           [4.4576642771203572, 0.27070040562690367], [4.4539131513204442, 0.26459170329581971],
           [4.3382111505607117, 0.24687463329474138], [4.310198679666855, 0.28755248562100372],
           [4.2039876642940843, 0.2073856855845127], [3.9470037945882956, 0.2195533777820366],
           [3.9331501676270872, 0.2170757349138713], [4.5754559683849827, 0.26104051494535652],
           [4.2642624936388858, 0.19806039583822335], [4.5706824416144167, 0.27627316735866453],
           [4.4952660621362002, 0.27310649929664987], [4.1280346095719329, 0.21412126562843511],
           [4.702107661837851, 0.28078101781508691], [4.3612377258111295, 0.28296063658099774],
           [4.3928360159817359, 0.25249196673860069], [4.619780948501055, 0.308154069215198],
           [4.6255249871371529, 0.2871838362068449], [3.9917933176305453, 0.26222080376928197],
           [4.5911222120587842, 0.24158314666593719], [4.7323234446926934, 0.30298831294431572],
           [4.6420309783345495, 0.2226996158065076], [4.4840065250137826, 0.18910050499027081],
           [4.2008837124418799, 0.27346584166180499], [4.6727721248598089, 0.24316912777726657],
           [4.0524404065125603, 0.23955358289112402], [4.407234079071233, 0.23175283114286901],
           [4.3775364779031118, 0.18237985713814994], [4.2137035812729859, 0.19829266512324442],
           [4.2863175472689159, 0.21053360041884009], [4.171320800465331, 0.21710492971449791],
           [4.3428413939541413, 0.26979959188143826], [4.2291382319118123, 0.21735515437717148],
           [3.8442972649742635, 0.22770777603899439], [4.5550476400078654, 0.27127738544251534],
           [4.3470917222354197, 0.24595788151158762], [4.6061413889254199, 0.34741230365716708],
           [4.4407467217443903, 0.2416820086245918], [4.8650348381635968, 0.3448855942250097],
           [4.0804587106328789, 0.22980474319519392], [4.7738903909391501, 0.30308364810029104],
           [4.8594613216938338, 0.28479726057085702], [4.2891543708669788, 0.23961359188378892],
           [4.1859480092601551, 0.28596837525129037], [4.5147162650347656, 0.28127338639690669],
           [4.5150852419476992, 0.31150608436530935], [4.4821644608445421, 0.2501738264832119],
           [4.322221342123826, 0.23775312208148783], [4.6166645646706836, 0.2975872663282963],
           [4.5597728443322723, 0.22550537039172283], [4.2596278124987874, 0.195042798286334],
           [4.0484484569058861, 0.24216712711692681], [4.1549022960549635, 0.26204695229492947],
           [4.1828934891666494, 0.22174690409508802], [4.6232745693284807, 0.26047600069145055],
           [4.2151789970727576, 0.29986329950353974], [3.7092940494276805, 0.2154669390581504],
           [4.394609520718717, 0.30005019719705783], [4.8349171290348689, 0.36137438153397011],
           [3.9198749419953778, 0.23564172243057793], [4.2153489465328038, 0.18442652487756284],
           [4.6325782356433409, 0.32299778539123947], [4.7043723773215396, 0.25466220653430705],
           [4.3786105171544172, 0.25908681181336363], [4.4291579860220835, 0.30186657696934777],
           [4.4506104245073308, 0.19413357258103586], [4.2441034343520352, 0.21703463965748204],
           [4.5293972189063387, 0.23388448053196575], [3.6743784469522964, 0.18918924966123038],
           [4.2622064387238847, 0.17677483151577048], [4.4653139420608472, 0.26705217788097302]]

if __name__ == '__main__':
    compute(classe1, classe3)
    #compute(classe1, classe2)
    #compute(classe2, classe3)
